{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NRC_affect_intensity_anger': 0.0,\n",
       " 'NRC_affect_intensity_anticipation': 0.0,\n",
       " 'NRC_affect_intensity_disgust': 0.0,\n",
       " 'NRC_affect_intensity_fear': 0.0,\n",
       " 'NRC_affect_intensity_joy': 0.0,\n",
       " 'NRC_affect_intensity_negative': 0.0,\n",
       " 'NRC_affect_intensity_positive': 0.0,\n",
       " 'NRC_affect_intensity_sadness': 0.295,\n",
       " 'NRC_affect_intensity_surprise': 0.0,\n",
       " 'NRC_affect_intensity_trust': 0.0,\n",
       " 'NRC_hashtag_emotion_anger': 1.226434937585,\n",
       " 'NRC_hashtag_emotion_anticipation': 0.50785495228,\n",
       " 'NRC_hashtag_emotion_disgust': 2.47091673723061,\n",
       " 'NRC_hashtag_emotion_fear': 1.2209224907264,\n",
       " 'NRC_hashtag_emotion_joy': 0.267771549361,\n",
       " 'NRC_hashtag_emotion_negative': 0.0,\n",
       " 'NRC_hashtag_emotion_positive': 0.0,\n",
       " 'NRC_hashtag_emotion_sadness': 0.0160741701215,\n",
       " 'NRC_hashtag_emotion_surprise': 0.0312992192982,\n",
       " 'NRC_hashtag_emotion_trust': 0.0,\n",
       " 'nrc_hashtag_sentiment_negative_bigram_words': 6,\n",
       " 'nrc_hashtag_sentiment_negative_unigram_words': 8,\n",
       " 'nrc_hashtag_sentiment_positive_bigram_score': 3.112,\n",
       " 'nrc_hashtag_sentiment_positive_unigram_score': 1.105,\n",
       " 'nrc_hashtag_sentiment_postive_bigram_words': 3,\n",
       " 'nrc_hashtag_sentiment_postive_unigram_words': 8,\n",
       " 'nrc_hastag_sentiment_negative_bigram_score': -3.169,\n",
       " 'nrc_hastag_sentiment_negative_unigram_score': -5.128,\n",
       " 'senti_wordnet_negative_score': -1.250595238095238,\n",
       " 'senti_wordnet_negative_words': 2,\n",
       " 'senti_wordnet_positive_score': 1.9322916666666667,\n",
       " 'senti_wordnet_postive_words': 8,\n",
       " 'sentiment140_negative_bigram_score': -6.843,\n",
       " 'sentiment140_negative_bigram_words': 4,\n",
       " 'sentiment140_negative_unigram_score': -3.7430000000000003,\n",
       " 'sentiment140_negative_unigram_words': 12,\n",
       " 'sentiment140_positive_bigram_score': 0.681,\n",
       " 'sentiment140_positive_unigram_score': 0.87,\n",
       " 'sentiment140_postive_bigram_words': 5,\n",
       " 'sentiment140_postive_unigram_words': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import gzip\n",
    "\n",
    "class LexiconFeaturizer(object):\n",
    "    \"\"\" A class that featurizes a tweet affect/sentiment lexicons\"\"\"\n",
    "\n",
    "    def __init__(self, list_of_featurizers=[]):\n",
    "        self.list_of_featurizers = []\n",
    "\n",
    "    def get_bigrams(self, tokens):\n",
    "        \"\"\" Return a list of bigram from a set of tokens \"\"\"\n",
    "        return [a + \" \" + b for a, b in zip(tokens, tokens[1:])]\n",
    "\n",
    "    def nrc_hashtag_emotion(self, tokens):\n",
    "        \"\"\" Build features using NRC Hashtag emotion dataset \"\"\"\n",
    "        nrc_hashtag_emotion_path = \"../data/lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt.gz\"\n",
    "        lexicon_map = defaultdict(list)\n",
    "\n",
    "        with gzip.open(nrc_hashtag_emotion_path, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            for l in lines[1:]:\n",
    "                splits = l.decode('utf-8').split('\\t')\n",
    "                lexicon_map[splits[0]] = [float(num) for num in splits[1:]]\n",
    "        num_features = 10  # 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust'\n",
    "        sum_vec = [0.0] * num_features\n",
    "        for token in tokens:\n",
    "            if token in lexicon_map:\n",
    "                sum_vec = [a + b for a, b in zip(sum_vec, lexicon_map[token])] # sum up the individual word feature vectors\n",
    "        feature_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
    "        feature_names = ['nrc_hashtag_emotion_' + name for name in feature_names]\n",
    "        return dict(zip(feature_names, sum_vec))\n",
    "\n",
    "    def nrc_affect_intensity(self, tokens):\n",
    "        \"\"\" Build feature vector using NRC affect intensity lexicons \"\"\"\n",
    "        nrc_affect_intensity_path = \"../data/lexicons/nrc_affect_intensity.txt.gz\"\n",
    "        lexicon_map = defaultdict(list)\n",
    "\n",
    "        with gzip.open(nrc_affect_intensity_path, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            for l in lines[1:]:\n",
    "                splits = l.decode('utf-8').split('\\t')\n",
    "                lexicon_map[splits[0]] = [float(num) for num in splits[1:]]\n",
    "\n",
    "        num_features = 10  # 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust'\n",
    "        sum_vec = [0.0] * num_features\n",
    "        for token in tokens:\n",
    "            if token in lexicon_map:\n",
    "                sum_vec = [a + b for a, b in zip(sum_vec, lexicon_map[token])]\n",
    "        \n",
    "        feature_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
    "        feature_names = ['NRC_affect_intensity_' + name for name in feature_names]\n",
    "        return dict(zip(feature_names, sum_vec))\n",
    "\n",
    "    def nrc_hashtag_sentiment_lexicon_unigrams(self, tokens):\n",
    "        \"\"\"\n",
    "        Function returns sum of intensities of\n",
    "        positive and negative tokens using only unigrams. Also returns\n",
    "        the number of positive and negative tokens\n",
    "        \"\"\"\n",
    "\n",
    "        nrc_hashtag_sentiment_lexicon_unigrams_path = \"../data/lexicons/NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt.gz\"\n",
    "        with gzip.open(nrc_hashtag_sentiment_lexicon_unigrams_path, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lexicon_map = {}\n",
    "            for l in lines:\n",
    "                splits = l.decode('utf-8').split('\\t')\n",
    "                lexicon_map[splits[0]] = float(splits[1])\n",
    "\n",
    "        positive_score, negative_score = 0.0, 0.0\n",
    "        positive_unigram_words, negative_unigram_words = 0, 0\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in lexicon_map:\n",
    "                if lexicon_map[token] >= 0:\n",
    "                    positive_score += lexicon_map[token]\n",
    "                    positive_unigram_words += 1\n",
    "                else:\n",
    "                    negative_score += lexicon_map[token]\n",
    "                    negative_unigram_words += 1\n",
    "\n",
    "        return {\n",
    "            \"nrc_hashtag_sentiment_positive_unigram_score\": positive_score, \n",
    "            \"nrc_hastag_sentiment_negative_unigram_score\": negative_score, \n",
    "            \"nrc_hashtag_sentiment_postive_unigram_words\": positive_unigram_words, \n",
    "            \"nrc_hashtag_sentiment_negative_unigram_words\": negative_unigram_words\n",
    "        }\n",
    "\n",
    "    def nrc_hashtag_sentiment_lexicon_bigrams(self, tokens):\n",
    "        \"\"\" \n",
    "        Function returns sum of intensities of \n",
    "        positive and negative tokens using only unigrams. Also returns\n",
    "        the number of positive and negative tokens\n",
    "        \"\"\"\n",
    "\n",
    "        nrc_hashtag_sentiment_lexicon_bigrams_path = \"../data/lexicons/NRC-Hashtag-Sentiment-Lexicon-v0.1/bigrams-pmilexicon.txt.gz\"\n",
    "        with gzip.open(nrc_hashtag_sentiment_lexicon_bigrams_path, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lexicon_map = {}\n",
    "            for l in lines:\n",
    "                splits = l.decode('utf-8').split('\\t')\n",
    "                lexicon_map[splits[0]] = float(splits[1])\n",
    "\n",
    "        positive_score, negative_score = 0.0, 0.0\n",
    "        positive_bigram_words, negative_bigram_words = 0, 0\n",
    "\n",
    "        # loop through the bigrams\t\n",
    "        for token in self.get_bigrams(tokens):\n",
    "            if token in lexicon_map:\n",
    "                if lexicon_map[token] >= 0:\n",
    "                    positive_score += lexicon_map[token]\n",
    "                    positive_bigram_words += 1\n",
    "                else:\n",
    "                    negative_score += lexicon_map[token]\n",
    "                    negative_bigram_words += 1\n",
    "        return {\n",
    "            \"nrc_hashtag_sentiment_positive_bigram_score\": positive_score, \n",
    "            \"nrc_hastag_sentiment_negative_bigram_score\": negative_score, \n",
    "            \"nrc_hashtag_sentiment_postive_bigram_words\": positive_bigram_words, \n",
    "            \"nrc_hashtag_sentiment_negative_bigram_words\": negative_bigram_words\n",
    "        }\n",
    "\n",
    "    def sentiment140_unigrams(self, tokens):\n",
    "        \"\"\" Sentiment 140 Unigram Lexicons features \"\"\"\n",
    "        sentiment140_unigrams = \"../data/lexicons/Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt.gz\"\n",
    "        with gzip.open(sentiment140_unigrams, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lexicon_map = {}\n",
    "            for l in lines:\n",
    "                splits = l.decode('utf-8').split('\\t')\n",
    "                lexicon_map[splits[0]] = float(splits[1])\n",
    "\n",
    "        positive_score, negative_score = 0.0, 0.0\n",
    "        positive_unigram_words, negative_unigram_words = 0, 0\t\n",
    "\n",
    "        # loop through the bigrams\t\n",
    "        for token in tokens:\n",
    "            if token in lexicon_map:\n",
    "                if lexicon_map[token] >= 0:\n",
    "                    positive_score += lexicon_map[token]\n",
    "                    positive_unigram_words += 1\n",
    "                else:\n",
    "                    negative_score += lexicon_map[token]\n",
    "                    negative_unigram_words += 1\n",
    "        return {\n",
    "            \"sentiment140_positive_unigram_score\": positive_score, \n",
    "            \"sentiment140_negative_unigram_score\": negative_score, \n",
    "            \"sentiment140_postive_unigram_words\": positive_unigram_words, \n",
    "            \"sentiment140_negative_unigram_words\": negative_unigram_words\n",
    "        }\n",
    "\n",
    "    def sentiment140_bigrams(self, tokens):\n",
    "        \"\"\" Sentiment 140 Unigram Lexicons features \"\"\"\n",
    "        sentiment140_bigrams = \"../data/lexicons/Sentiment140-Lexicon-v0.1/bigrams-pmilexicon.txt.gz\"\n",
    "        with gzip.open(sentiment140_bigrams, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            lexicon_map = {}\n",
    "            for l in lines:\n",
    "                splits = l.decode('utf-8').split('\\t')\n",
    "                lexicon_map[splits[0]] = float(splits[1])\n",
    "\n",
    "        positive_score, negative_score = 0.0, 0.0\n",
    "        positive_bigram_words, negative_bigram_words = 0, 0\t\n",
    "\n",
    "        # loop through the bigrams\t\n",
    "        for token in self.get_bigrams(tokens):\n",
    "            if token in lexicon_map:\n",
    "                if lexicon_map[token] >= 0:\n",
    "                    positive_score += lexicon_map[token]\n",
    "                    positive_bigram_words += 1\n",
    "                else:\n",
    "                    negative_score += lexicon_map[token]\n",
    "                    negative_bigram_words += 1\n",
    "\n",
    "        return {\n",
    "            \"sentiment140_positive_bigram_score\": positive_score, \n",
    "            \"sentiment140_negative_bigram_score\": negative_score, \n",
    "            \"sentiment140_postive_bigram_words\": positive_bigram_words, \n",
    "            \"sentiment140_negative_bigram_words\": negative_bigram_words\n",
    "        }\n",
    "    \n",
    "    def senti_wordnet(self, tokens):\n",
    "        \"\"\" Returns features based on the SentiWordNet features \"\"\"\n",
    "\n",
    "        senti_wordnet_path = \"../data/lexicons/SentiWordNet_3.0.0.txt.gz\"\n",
    "        with gzip.open(senti_wordnet_path, 'rb') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            senti_wordnet_lexicon_map = defaultdict(float)\n",
    "\n",
    "            for l in lines:\n",
    "                l = l.decode('utf-8')\n",
    "                if l.strip().startswith('#'):\n",
    "                    continue\n",
    "                splits = l.split('\\t')\n",
    "                # positive score - negative score\n",
    "                score = float(splits[2]) - float(splits[3])\n",
    "                words = splits[4].split(\" \")\n",
    "                # iterate through all words\n",
    "                for word in words:\n",
    "                    word, rank = word.split('#')\n",
    "                    # scale scores according to rank\n",
    "                    # more popular => less rank => high weight\n",
    "                    senti_wordnet_lexicon_map[word] += (score / float(rank))\n",
    "\n",
    "        positive_score, negative_score = 0.0, 0.0\n",
    "        positive_unigram_words, negative_unigram_words = 0, 0\n",
    "\n",
    "        # loop through the bigrams\t\n",
    "        for token in tokens:\n",
    "            if token in senti_wordnet_lexicon_map:\n",
    "                if senti_wordnet_lexicon_map[token] >= 0:\n",
    "                    positive_score += senti_wordnet_lexicon_map[token]\n",
    "                    positive_unigram_words += 1\n",
    "                else:\n",
    "                    negative_score += senti_wordnet_lexicon_map[token]\n",
    "                    negative_unigram_words += 1\n",
    "\n",
    "        return  {\n",
    "            \"senti_wordnet_positive_score\": positive_score, \n",
    "            \"senti_wordnet_negative_score\": negative_score, \n",
    "            \"senti_wordnet_postive_words\": positive_unigram_words, \n",
    "            \"senti_wordnet_negative_words\": negative_unigram_words\n",
    "        }\n",
    "\n",
    "    def featurize(self, tokens):\n",
    "        \"\"\" Build a feature vector for the tokens \"\"\"\n",
    "        features = {}\n",
    "        nrc_hashtag_emotion_features = self.nrc_hashtag_emotion(tokens)\n",
    "        nrc_affect_intensity_features = self.nrc_affect_intensity(tokens)\n",
    "        nrc_hashtag_sentiment_lexicon_unigrams_features = self.nrc_hashtag_sentiment_lexicon_unigrams(tokens)\n",
    "        nrc_hashtag_sentiment_lexicon_bigrams_features = self.nrc_hashtag_sentiment_lexicon_bigrams(tokens)\n",
    "        sentiment140_unigrams_features = self.sentiment140_unigrams(tokens)\n",
    "        sentiment140_bigrams_features = self.sentiment140_bigrams(tokens)\n",
    "        senti_wordnet_features = self.senti_wordnet(tokens)\n",
    "\n",
    "        features.update(nrc_hashtag_emotion_features) # 10 features\n",
    "        features.update(nrc_affect_intensity_features) # 10 features\n",
    "        features.update(nrc_hashtag_sentiment_lexicon_unigrams_features) # 4 features\n",
    "        features.update(nrc_hashtag_sentiment_lexicon_bigrams_features) # 4 features\n",
    "        features.update(sentiment140_unigrams_features) # 4 features \n",
    "        features.update(sentiment140_bigrams_features) # 4 features\n",
    "        features.update(senti_wordnet_features) # 4 features\n",
    "\n",
    "        return features\n",
    "\n",
    "# Example of using the featurizer\n",
    "def test_featurizer():\n",
    "    featurizer = LexiconFeaturizer()\n",
    "    features = featurizer.featurize([u'So', u'my', u'Indian', u'Uber', u'driver', u'just', u'called', u'someone', u'the', u'N', u'word', u'.', u'If', u'I', u\"wasn't\", u'in', u'a', u'moving', u'vehicle', u\"I'd\", u'have', u'jumped', u'out', u'#disgusted'])\n",
    "    return features\n",
    "\n",
    "test_featurizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
